{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import emoji_dict, stop_words, nouns, adjectives\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_16      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_16        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">24,516,864</span> │ input_layer_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">918,528</span> │ embedding_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_14        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_15… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ bidirectional_15… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_16      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_16        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │ \u001b[38;5;34m24,516,864\u001b[0m │ input_layer_16[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m918,528\u001b[0m │ embedding_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_14        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional_15… │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ bidirectional_15… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ attention_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │        \u001b[38;5;34m387\u001b[0m │ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,468,678</span> (97.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,468,678\u001b[0m (97.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">951,811</span> (3.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m951,811\u001b[0m (3.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,516,864</span> (93.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m24,516,864\u001b[0m (93.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> (16.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3\u001b[0m (16.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = load_model('model/bert_attention_v6.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/file/d/1O6Nyg8yI4Kb-VaOlZr33_zkDGBDMrfwg/view?usp=drive_link'\n",
    "url ='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "train_df = pd.read_csv(url)\n",
    "\n",
    "url = 'https://drive.google.com/file/d/1EeIIXUOEQDaHvTDGGvpa61BNVe-ggJIV/view?usp=drive_link'\n",
    "url ='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "valid_df = pd.read_csv(url)\n",
    "\n",
    "url = 'https://drive.google.com/file/d/1Yh4CMGWT-ERZ6T7Q7JPVf2ZjGZx9Sh_O/view?usp=drive_link'\n",
    "url ='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "test_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['reviews'] = train_df['reviews'].astype(str)\n",
    "valid_df['reviews'] = valid_df['reviews'].astype(str)\n",
    "test_df['reviews'] = test_df['reviews'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/file/d/1-wpLUWpIVBTr0BLHG79cZhHrZ34KFOJs/view?usp=drive_link'\n",
    "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "data_df = pd.read_csv(url)\n",
    "data_df['reviews'] = data_df['reviews'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 13140\n",
      "Length of validation data: 1402\n",
      "Length of test data: 463\n"
     ]
    }
   ],
   "source": [
    "data_df.columns = train_df.columns\n",
    "\n",
    "train_data_df, valid_data_df = train_test_split(data_df, train_size=4000, random_state=42)\n",
    "\n",
    "train_df = pd.concat([train_df, train_data_df], ignore_index=True)\n",
    "valid_df = pd.concat([valid_df, valid_data_df], ignore_index=True)\n",
    "\n",
    "print(f\"Length of training data: {len(train_df)}\")\n",
    "print(f\"Length of validation data: {len(valid_df)}\")\n",
    "print(f\"Length of test data: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung dimiliki pengusaha pabrik puluhan terke...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lokasi strategis jalan sumatera bandung nya ny...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>betapa bahagia nya unboxing paket barang nya b...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>makanan beragam harga makanan food stall kasir...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pakai kartu kredit bca tidak untung rugi besar</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13135</th>\n",
       "      <td>masker camille emang bagus woii lahh emang mas...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13136</th>\n",
       "      <td>ready kah</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13137</th>\n",
       "      <td>emang langsung putih tpi jgn coba kulit badak ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13138</th>\n",
       "      <td>orang jateng cocok bgt manissss</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13139</th>\n",
       "      <td>kecewa bangetttttt beli mie goreng pas diliat ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13140 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  negative  neutral  \\\n",
       "0      warung dimiliki pengusaha pabrik puluhan terke...     False    False   \n",
       "1      lokasi strategis jalan sumatera bandung nya ny...     False    False   \n",
       "2      betapa bahagia nya unboxing paket barang nya b...     False    False   \n",
       "3      makanan beragam harga makanan food stall kasir...     False    False   \n",
       "4         pakai kartu kredit bca tidak untung rugi besar      True    False   \n",
       "...                                                  ...       ...      ...   \n",
       "13135  masker camille emang bagus woii lahh emang mas...     False    False   \n",
       "13136                                          ready kah     False     True   \n",
       "13137  emang langsung putih tpi jgn coba kulit badak ...     False    False   \n",
       "13138                    orang jateng cocok bgt manissss     False    False   \n",
       "13139  kecewa bangetttttt beli mie goreng pas diliat ...      True    False   \n",
       "\n",
       "       positive  \n",
       "0          True  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4         False  \n",
       "...         ...  \n",
       "13135      True  \n",
       "13136     False  \n",
       "13137      True  \n",
       "13138      True  \n",
       "13139     False  \n",
       "\n",
       "[13140 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kamus_alay_df = pd.read_csv('https://raw.githubusercontent.com/nasalsabila/kamus-alay/refs/heads/master/colloquial-indonesian-lexicon.csv')\n",
    "# kamus_alay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kamus_alay_dict = kamus_alay_df.set_index('slang')['formal'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def df_to_dataset(df, shuffle=True, batch_size=32):\n",
    "#     \"\"\"\n",
    "#     Converts a pandas DataFrame into a TensorFlow Dataset.\n",
    "#     - `reviews` is the text input.\n",
    "#     - Target columns are `negative`, `neutral`, and `positive`.\n",
    "#     \"\"\"\n",
    "#     features = df['reviews']  # Input text\n",
    "#     labels = df[['negative', 'neutral', 'positive']]  # Multi-class targets\n",
    "#     ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "#     if shuffle:\n",
    "#         ds = ds.shuffle(buffer_size=len(df))\n",
    "#     ds = ds.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "#     return ds\n",
    "\n",
    "# # Convert the datasets\n",
    "# batch_size = 32\n",
    "# train_ds = df_to_dataset(train_df, shuffle=True, batch_size=batch_size)\n",
    "# valid_ds = df_to_dataset(valid_df, shuffle=False, batch_size=batch_size)\n",
    "# test_ds = df_to_dataset(test_df, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(reviews, label):\n",
    "#     \"\"\"\n",
    "#     Preprocess the reviews using kamus_alay_dict.\n",
    "#     Replace words if found in the dictionary; otherwise, keep the original word.\n",
    "#     \"\"\"\n",
    "#     # Split the reviews into words\n",
    "#     words = tf.strings.split(reviews)\n",
    "\n",
    "#     # Map the dictionary lookup to each word\n",
    "#     def replace_word(word):\n",
    "#         # Use a Python dictionary lookup wrapped in tf.py_function\n",
    "#         def lookup(w):\n",
    "#             return kamus_alay_dict.get(w, w)  # Replace with dict value or keep the same\n",
    "        \n",
    "#         # Apply the lookup function and return\n",
    "#         return tf.py_function(func=lookup, inp=[word], Tout=tf.string)\n",
    "\n",
    "#     # Replace each word using map_fn\n",
    "#     words = tf.map_fn(replace_word, words, fn_output_signature=tf.string)\n",
    "\n",
    "#     # Join words back into a string\n",
    "#     processed_review = tf.strings.reduce_join(words, separator=\" \")\n",
    "\n",
    "#     return processed_review, label\n",
    "\n",
    "\n",
    "\n",
    "# train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# valid_ds = valid_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def tokenize(text):\n",
    "#     # Tokenize the text using the tokenizer\n",
    "#     return tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"tf\")['input_ids']\n",
    "\n",
    "# # Apply tokenization to each review\n",
    "# X_train_tokenized = tf.convert_to_tensor(\n",
    "#     [tokenize(text) for text in train_df['reviews']],\n",
    "#     dtype=tf.int32\n",
    "# )\n",
    "\n",
    "# X_valid_tokenized = tf.convert_to_tensor(\n",
    "#     [tokenize(text) for text in valid_df['reviews']],\n",
    "#     dtype=tf.int32\n",
    "# )\n",
    "\n",
    "# X_test_tokenized = tf.convert_to_tensor(\n",
    "#     [tokenize(text) for text in test_df['reviews']],\n",
    "#     dtype=tf.int32\n",
    "# )\n",
    "\n",
    "# # Convert labels to tensors\n",
    "# y_train = tf.convert_to_tensor(train_df.iloc[:, 1:].values, dtype=tf.int32)\n",
    "# y_valid = tf.convert_to_tensor(valid_df.iloc[:, 1:].values, dtype=tf.int32)\n",
    "# y_test = tf.convert_to_tensor(test_df.iloc[:, 1:].values, dtype=tf.int32)\n",
    "\n",
    "# X_train_tokenized = tf.reshape(X_train_tokenized, (-1, 128))\n",
    "# X_valid_tokenized = tf.reshape(X_valid_tokenized, (-1, 128))\n",
    "# X_test_tokenized = tf.reshape(X_test_tokenized, (-1, 128))\n",
    "\n",
    "# # Check the shapes of the tokenized datasets\n",
    "\n",
    "# print(X_train_tokenized.shape)\n",
    "# print(X_valid_tokenized.shape)\n",
    "# print(X_test_tokenized.shape)\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"tf\")['input_ids']\n",
    "\n",
    "X_train_tokenized = tf.squeeze(tf.stack(train_df['reviews'].map(lambda x: tokenize(x)).tolist()), axis=1)\n",
    "X_valid_tokenized = tf.squeeze(tf.stack(valid_df['reviews'].map(lambda x: tokenize(x)).tolist()), axis=1)\n",
    "X_test_tokenized = tf.squeeze(tf.stack(test_df['reviews'].map(lambda x: tokenize(x)).tolist()), axis=1)\n",
    "\n",
    "y_train = tf.convert_to_tensor(train_df.iloc[1:, 1:].values)\n",
    "y_valid = tf.convert_to_tensor(valid_df.iloc[:, 1:].values)\n",
    "y_test = tf.convert_to_tensor(test_df.iloc[:, 1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.718809  , -5.3317833 ,  0.3994115 ],\n",
       "       [ 2.76671   , -4.195007  , -0.4184649 ],\n",
       "       [ 3.2485332 , -2.509388  , -2.4076605 ],\n",
       "       ...,\n",
       "       [-1.1843995 ,  1.0652477 ,  0.67548263],\n",
       "       [-1.2078623 ,  0.06133045,  1.4901582 ],\n",
       "       [-0.02313354, -0.8224718 ,  0.96673864]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_tokenized)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def get_top_3_positive_index(predictions):\n",
    "    positive_preds = predictions[:, 2]\n",
    "    pq = []\n",
    "    for i, pred in enumerate(positive_preds):\n",
    "        if np.argmax(predictions[i]) == 2:\n",
    "            heapq.heappush(pq, (-pred, i))\n",
    "    top_3 = []\n",
    "    for _ in range(3):\n",
    "        if pq:\n",
    "            _, task = heapq.heappop(pq)\n",
    "            top_3.append(task)\n",
    "    return top_3\n",
    "\n",
    "def get_top_3_negative_index(predictions):\n",
    "    negative_preds = predictions[:, 0]\n",
    "    pq = []\n",
    "    for i, pred in enumerate(negative_preds):\n",
    "        if np.argmax(predictions[i]) == 0:\n",
    "            heapq.heappush(pq, (-pred, i))\n",
    "    top_3 = []\n",
    "    for _ in range(3):\n",
    "        if pq:\n",
    "            _, task = heapq.heappop(pq)\n",
    "            top_3.append(task)\n",
    "    return top_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_texts(indices, data):\n",
    "    texts = []\n",
    "    for index in indices:\n",
    "        texts.append(data[index])\n",
    "    return texts\n",
    "\n",
    "def get_top_3_positive_comments(predictions, data):\n",
    "    top_3_positive_idx = get_top_3_positive_index(predictions)\n",
    "    comments = indices_to_texts(top_3_positive_idx, data)\n",
    "\n",
    "    return comments\n",
    "\n",
    "def get_top_3_negative_comments(predictions, data):\n",
    "    top_3_negative_idx = get_top_3_negative_index(predictions)\n",
    "    comments = indices_to_texts(top_3_negative_idx, data)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step\n",
      "['woi orangorang kampung khusus nya daerah kalimantan barat bakar ladang kebon apalah pikir dampak nya ganggu penerbangan tolol babi bangsat mata dibuka woi orang kampung', 'ustad bajingan ustadtengkuzul kampanye sara kritik berita hoax si bajingan hadits palsu ceramah rokok kencing iblis', 'murahan opini sampah diusung pendukung korup']\n",
      "['bandung lewatkan ngafe valey dago pakar menikmati makan malam menikmati pemandangan bagus makanan nya enak coba menginap hotel nya berbentuk caravan asyik banget udara nya segar sarapan nya mantap linting capgomeh nya enak banget', 'salah favorit keluarga lumayan datangi menu beragam pilihan bento ramen toast sushi nya oke interior ruang cukup nyaman bersantai bareng keluarga pelayanan cukup ramah harga terjangkau mantap', 'suasana resto cukup nyaman layanan cukup baik menawarkan beragam menu bahan dasar seafood udang gurame bawal kakap kepiting ayam sayuran gurame goreng gurih renyah sambal mangga nya enak nikmat ayam goreng fillet nya gurih enak gurame bakar kecap dibakar sempurna gurih ratarata masakan nya enak']\n"
     ]
    }
   ],
   "source": [
    "valid_predictions = model.predict(X_valid_tokenized)\n",
    "print(get_top_3_negative_comments(valid_predictions, valid_df['reviews']))\n",
    "print(get_top_3_positive_comments(valid_predictions, valid_df['reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gue pesimis manusia ngurusin sampah hewanhewan laut korban garagara buang sampah laut tidak tega gue liat nya kadang tidak mengerti pemimpinpemimpin mikirin sih urusan penting kayak gini he',\n",
       " 'kecewa pelayanan hotel semalam tempati kamar mandi kotor pelayan tidak ramah',\n",
       " 'rakyat indonesia kecewa settingan kayak sinetron asli rakyat indonesia kecewa tidak nonton asian games sctv besar rakyat indonesia india sinetron tidak jelas']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_3_negative_comments(predictions, test_df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['menginap malam keluarga hotel nya bagus bersih fasilitas hotel lengkap breakfast nya enakenak suasana hotel sejuk nyaman cocok pilihan menginap keluarga',\n",
       " 'setia traveloka pilihan maskapai nya payment nya mudah selaint harga nya murah mantap',\n",
       " 'memakai berenam memakai kamar kamar besar bersih breakfast nya enak varian nya aneka segar juice pelayanan baik pelayanan restoran memuaskan rekomendasi']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_3_positive_comments(predictions, test_df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kecewa aplikasi hubungi pusat panggilan cek pemesanan bayar tidak muncul e tiket nya kirim bukti transaksi nya email tidak pusat panggilan menghubungi kecewa'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['reviews'][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_class(predictions):\n",
    "    m, n = predictions.shape\n",
    "    y_pred_class = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        max_val = 1e-3\n",
    "        max_j = -1\n",
    "        for j in range(n):\n",
    "            if max_val < predictions[i, j]:\n",
    "                max_val = predictions[i, j]\n",
    "                max_j = j\n",
    "        y_pred_class[i, max_j] = 1\n",
    "    y_pred_class = y_pred_class.astype(bool)\n",
    "    y_pred_class = np.argwhere(y_pred_class)[:, 1]\n",
    "    return y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 2, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       1, 2, 1, 1, 0, 0, 1, 1, 0, 2, 2, 1, 1, 1, 2, 0, 1, 0, 0, 1, 0, 1,\n",
       "       2, 0, 0, 1, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 1, 2,\n",
       "       2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_class = get_prediction_class(predictions)\n",
    "predictions_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred = np.max(predictions, axis=1)\n",
    "# max_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 16:44:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 7.15MB/s]                    \n",
      "2024-11-25 16:44:18 INFO: Downloaded file to ./stanza_models\\resources.json\n",
      "INFO:stanza:Downloaded file to ./stanza_models\\resources.json\n",
      "2024-11-25 16:44:20 INFO: Loading these models for language: id (Indonesian):\n",
      "===============================\n",
      "| Processor    | Package      |\n",
      "-------------------------------\n",
      "| tokenize     | gsd          |\n",
      "| mwt          | gsd          |\n",
      "| pos          | gsd_charlm   |\n",
      "| lemma        | gsd_nocharlm |\n",
      "| constituency | icon_charlm  |\n",
      "| depparse     | gsd_charlm   |\n",
      "===============================\n",
      "\n",
      "INFO:stanza:Loading these models for language: id (Indonesian):\n",
      "===============================\n",
      "| Processor    | Package      |\n",
      "-------------------------------\n",
      "| tokenize     | gsd          |\n",
      "| mwt          | gsd          |\n",
      "| pos          | gsd_charlm   |\n",
      "| lemma        | gsd_nocharlm |\n",
      "| constituency | icon_charlm  |\n",
      "| depparse     | gsd_charlm   |\n",
      "===============================\n",
      "\n",
      "2024-11-25 16:44:20 INFO: Using device: cpu\n",
      "INFO:stanza:Using device: cpu\n",
      "2024-11-25 16:44:20 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-25 16:44:26 INFO: Loading: mwt\n",
      "INFO:stanza:Loading: mwt\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-25 16:44:26 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-25 16:44:27 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-25 16:44:27 INFO: Loading: constituency\n",
      "INFO:stanza:Loading: constituency\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\constituency\\base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-25 16:44:28 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-25 16:44:29 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize the Indonesian pipeline\n",
    "custom_dir = \"./stanza_models\"\n",
    "\n",
    "nlp = stanza.Pipeline('id', dir=custom_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common nouns/objects in positive predictions: [('produk', 1), ('pelayanan', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences and one-hot encoded predictions\n",
    "texts = [\"Saya suka produk ini\", \"Ini sangat buruk\", \"Pelayanan yang luar biasa!\", \"Pembelian ini biasa saja\"]\n",
    "predictions = np.array([\n",
    "    [0, 0, 1],  # Positive\n",
    "    [1, 0, 0],  # Negative\n",
    "    [0, 0, 1],  # Positive\n",
    "    [0, 1, 0],  # Neutral\n",
    "])\n",
    "\n",
    "class_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "for text, label in zip(texts, class_labels):\n",
    "    if label == 2:  # Positive sentiment\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Extract nouns (objects)\n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                if word.upos == \"NOUN\":  # Check if the word is a noun\n",
    "                    word_counter[word.text.lower()] += 1\n",
    "\n",
    "# Most common nouns/objects in positive predictions\n",
    "most_common_words = word_counter.most_common()\n",
    "print(\"Most common nouns/objects in positive predictions:\", most_common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_tokenized)\n",
    "texts = test_df['reviews'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 16:44:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 1.43MB/s]                    \n",
      "2024-11-25 16:44:34 INFO: Downloaded file to ./stanza_models\\resources.json\n",
      "INFO:stanza:Downloaded file to ./stanza_models\\resources.json\n",
      "2024-11-25 16:44:36 INFO: Loading these models for language: id (Indonesian):\n",
      "===============================\n",
      "| Processor    | Package      |\n",
      "-------------------------------\n",
      "| tokenize     | gsd          |\n",
      "| mwt          | gsd          |\n",
      "| pos          | gsd_charlm   |\n",
      "| lemma        | gsd_nocharlm |\n",
      "| constituency | icon_charlm  |\n",
      "| depparse     | gsd_charlm   |\n",
      "===============================\n",
      "\n",
      "INFO:stanza:Loading these models for language: id (Indonesian):\n",
      "===============================\n",
      "| Processor    | Package      |\n",
      "-------------------------------\n",
      "| tokenize     | gsd          |\n",
      "| mwt          | gsd          |\n",
      "| pos          | gsd_charlm   |\n",
      "| lemma        | gsd_nocharlm |\n",
      "| constituency | icon_charlm  |\n",
      "| depparse     | gsd_charlm   |\n",
      "===============================\n",
      "\n",
      "2024-11-25 16:44:36 INFO: Using device: cpu\n",
      "INFO:stanza:Using device: cpu\n",
      "2024-11-25 16:44:36 INFO: Loading: tokenize\n",
      "INFO:stanza:Loading: tokenize\n",
      "d:\\COURSE\\Computer science\\Bangkit\\Project\\GradioProto\\venv\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-25 16:44:36 INFO: Loading: mwt\n",
      "INFO:stanza:Loading: mwt\n",
      "2024-11-25 16:44:36 INFO: Loading: pos\n",
      "INFO:stanza:Loading: pos\n",
      "2024-11-25 16:44:37 INFO: Loading: lemma\n",
      "INFO:stanza:Loading: lemma\n",
      "2024-11-25 16:44:37 INFO: Loading: constituency\n",
      "INFO:stanza:Loading: constituency\n",
      "2024-11-25 16:44:38 INFO: Loading: depparse\n",
      "INFO:stanza:Loading: depparse\n",
      "2024-11-25 16:44:39 INFO: Done loading processors!\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "custom_dir = \"./stanza_models\"\n",
    "\n",
    "nlp = stanza.Pipeline('id', dir=custom_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedaaa\n",
    "import re\n",
    "def replace_emoji_with_word(text):\n",
    "    for emoji, word in emoji_dict.items():\n",
    "        text = re.sub(f'({emoji})+', f' {word} ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def stop_words_removal(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    removed_stop_words = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(removed_stop_words)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = replace_emoji_with_word(text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "    text = stop_words_removal(text)\n",
    "    return text\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(\n",
    "        texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"tf\"\n",
    "    )['input_ids']\n",
    "\n",
    "# PREDICT\n",
    "def predict_sentiment_batch(texts):\n",
    "    # preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "    tokenized_texts = tokenize_batch(texts)\n",
    "    predictions = model.predict(tokenized_texts)\n",
    "    sentiment_labels = [\"Negatif\", \"Netral\", \"Positif\"]\n",
    "    sentiments = np.argmax(predictions, axis=1)\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = predict_sentiment_batch(test_df['reviews'])\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step\n",
      "Yang disukai customer: [('cheese enak', 2), ('cheese kaya', 1), ('false sopan', 1), ('raya langsung', 1), ('grabid enak', 1), ('video sedih', 1), ('email good', 1), ('service bagus', 1), ('wawww seru', 1), ('keluarga kaget', 1), ('setujuuu bagus', 1), ('omg enak', 1), ('promo enak', 1), ('chesse enak', 1), ('crust cocok', 1), ('nagih keren', 1), ('nyesel senyum', 1), ('cinta sedih', 1), ('apresiasi bagus', 1), ('kaget enak', 1), ('voucher favorit', 1), ('nih enak', 1), ('promosi ribet', 1), ('nama nyakk', 1)]\n",
      "Yang tidak disukai customer: [('ai parah', 1), ('pelayanan buruk', 1), ('dech murah', 1), ('oulet mahal', 1), ('jarak jauh', 1), ('mojokerto buruk', 1), ('karyawan higienis', 1), ('pekerjaan habis', 1), ('tangan langsung', 1), ('cinta lama', 1), ('pesanan sedih', 1), ('adonan kering', 1), ('dih jauh', 1), ('pekan sedih', 1), ('pontianak sedih', 1), ('saosnya parah', 1), ('pesanan repot', 1), ('refund ribet', 1), ('niaga salah', 1), ('bs diskon', 1), ('kasir sedih', 1), ('menunya sedih', 1), ('pizza viral', 1), ('nama sedih', 1), ('malam semelimpah', 1), ('pizza favorit', 1), ('trippinjann kaya', 1), ('bgt beda', 1), ('dominos_id liat', 1), ('ken langsung', 1), ('rebu beda', 1), ('yak beda', 1), ('harga jauh', 1), ('keju dikit', 1), ('pizza dingin', 1), ('video beda', 1), ('event parah', 1), ('porsi kecil', 1), ('pesan persis', 1), ('isian sesuai', 1), ('pizza kecil', 1)]\n"
     ]
    }
   ],
   "source": [
    "# The texts to predict\n",
    "texts = ['kembalikan philly cheese steak min',\n",
    "       'halo @dominos_id aku mau COMPLAIN ya... pesen lewat aplikasi wa, data uda sukses pemesanan take away karena buru2... daaaaan sampe store pesenanku ga ada dooong... orang store toko bilang ga ada masuk, sedangkan aku uda pesen via wa yg tercantum disini, kecewa bgtt ai parah, niat buru2 biar lebih simple, aplikasi ga bisa dipake mesen juga bolak balik donlot ulang tetep aja aplikasi error, tolooooong yaaaa... pelayanan buruk, bad bad bad',\n",
    "       'Sayangnya pakai app tinggal klik kirim ke alamat ga bisa2 bahkan udh di outlet nya tetep ga bisa ya sudah nyerah dech',\n",
    "       'Papi duo masih 57rban min ?',\n",
    "       'App lebih murah jangan mau datanh ke oulet dah lebih mahal ngantri pula kena zonk 😂😂😂',\n",
    "       'Min beda ht, tc, crunchy, pan, hm aplgi ya? Btw itu gmn si apa',\n",
    "       'Ya sy baru nyoba enaknya mmg 👍👍👍ga cuman iklan sj tp top enak',\n",
    "       '@dominos_id okk min trimakasih',\n",
    "       '@dominos_id emang pengantaran 5menit jarak nya terlalu jauh ya? Emang terjauh jarak nya berapa menit',\n",
    "       'Kak tolong cek dm ya', 'Baru nyoba kemarin, beneran enak 🤤☺️',\n",
    "       'Pelayanan @dominos_id mojokerto sangat sangat buruk',\n",
    "       'Fav banget pizza cheese kaya gini 😍🤤',\n",
    "       'Izin mau kasi saran...  Untuk store yg di pematang siantar tolong bilang ke karyawan nya agar lebih higienis lagi dalam melakukan pekerjaan..  Habis hitung uang sebaiknya cuci tangan dulu untuk mengerjakan pizza yg mau di panggang..  Sudahlah tidak pakai sarung tangan..  Langsung di depan mata lihat begitu jadi iuh... Mual @dominos_id',\n",
    "       '@dominos_id oh ini ada min struk nya, saya kirim dr DM yah.. Baru saya cr di tas masih ada',\n",
    "       'Sumpah enak banget loh yg rasa ini 😍',\n",
    "       'lama banget pesanannya jadi 😢', 'Bangkeee',\n",
    "       'Kecewa..perdana rasain krain enk bgt ..beli di mobil dbireuen yg bakal buka dilhokseumawe aceh, udh bli nya gk sesuai dksi..',\n",
    "       'Min cek dm saya beli pizza GK ada saos nya',\n",
    "       'Mau tanya apakah peraturan skrg klo mau saos sambal kena charge Rp 500/sachet?',\n",
    "       'Siang @dominos_id berapa hari yang lalu saya ada beli Pizza 2 box.... Pizza nya bantet adonan nya dan dalam nya kering.... Saya sangat kecewa. Saya ada Foto Pizza nya.',\n",
    "       '@dominos_id saya ada dm...silahkan di lihat foto nya....',\n",
    "       '@dominos_id uk brp kak', 'AYE DOMINO',\n",
    "       'kok ga ada menu nya min di aplikasi? di papi duo promo juga gak ada',\n",
    "       'Sama ini is @istiqomahbalya', \"Wa domino's gaada ya\",\n",
    "       'Kenapa ngk buka d pangkalan kerinci dih jauh bnget mau beli ke pekan 😢😢',\n",
    "       'Promo scan and win bohongan sudah dua kali beli domino ga bs dscan bilangnya false terus',\n",
    "       '@romaarmdhn15 🙏🙏',\n",
    "       'Kak aplikasi dominos pizza lg error kah kok aku gabisa dpt kode otp drtd 😭',\n",
    "       '@sriellagisti27 pdhl pangkalan kerinci teramat ramai ya kak😂 gpp ke pku aja beli nya, cuma 4jam-an kok kak😂',\n",
    "       'Aplikasinya sampah bgt si. Gabisa dibuka dr pagi buset',\n",
    "       'plis buka cabang di pontianak😢',\n",
    "       '@dominos_id buka di KalBar dong 🙏🥺',\n",
    "       'Admin buat tutorial donk, gmna caranya makan pizza medium dgn 6 potong irisan tapi saosnya cuman dikasih 2 😂😂',\n",
    "       'Buka cabang di banyuwangi jatim dong kak',\n",
    "       'Parah banget resto jakarta pusat ga bisa bantu cancel padahal rumah saya di Citra Raya Tangerang...habis aku di omelin Drive',\n",
    "       'bisa beli langsung?',\n",
    "       'Ini gimana ya saya pesen dominos sawangan, status nya belum terkirim malah udah ke kirim status nya dan orderan udah selesai, disuruh kontak ke dominos nya, mereka bilang ga ada pesanan, repot banget, refund juga ribet. Pelayanannya gimana sih? Malah dilempar2, buang waktu tenaga @dominos_id @grabfoodid @grabid',\n",
    "       'Bikin Domino X Keju Recheeseeeee', 'Buka di Cilacap dong min',\n",
    "       'Enak sih tapi gak Kya di video ini 😢', '@dominos_id gak di read',\n",
    "       'Skr kan lagi hut bank CIMB niaga yg mana salah merchant yg di infokan utk dapat cashback adl dominos, tapi harus pake qris via bank mandiri, nah tadi pihak kasir alesan mulu alat dr mandiri ga bisa, krna dia tau klo pake itu bs dapat diskon lagi, gmn siy tu kasir',\n",
    "       'Sedih banget american all star nya udah gak ada.. padahal itu yang paling favorite! Menunya baru semua ya dominos???? Please kembalikan american all star ku 😢😢😢😢😢',\n",
    "       'Promo Oktober mana min?',\n",
    "       '@dominos_id kak itu varian mac n chesee gitu ya',\n",
    "       'Payahh resto 1 @dominos_id ini ngk ada kmr mandi nya.... Yg ngalamin hal samarinda kebelet pipis tpi bungkusan kita blm siap...........',\n",
    "       'Ini size medium apa reguler?',\n",
    "       'Ka domino pizza ada paket ulang tahun ga?', 'cek DM please',\n",
    "       'Atau yg ini ty @syifa_rfh',\n",
    "       '@dominos_id surprise in yg ultah aja ka tapi gak ngadain acara, klo di pi** kan ada',\n",
    "       '@dominos_id tnks kk, Spil minumanya kk yg enak',\n",
    "       '@dominos_id Spil minumanya kk', '@dominos_id iya kk',\n",
    "       '@dominos_id kalo beli 1 doang brapa ka itu',\n",
    "       'min, saya udh download aplikasi dominos. tp gak msk2 email verifikasinya di email',\n",
    "       '@dominos_id sudah saya email kak',\n",
    "       '@dominos_id good service 👍… saya langsung di telp oleh CS nya dan solved',\n",
    "       'Buat ank ku yg bentar lagi ulang tahun 😍',\n",
    "       '@dominos_id papi duo promo sampai kapan kak',\n",
    "       'Promonya masih ada min bulan oktober ini',\n",
    "       'Mau tanya dong ini outletnya buka jambrpa ya?',\n",
    "       '@dominos_id apa ka promonya', 'Ini harga perpizza brp yaa?',\n",
    "       'Min, nama pizza yg Viral dulu apa min? Pengen cobain tapi ngga tau namanya 😢',\n",
    "       'Bilang nya gmna min kalo mau promo itu ? Dismua gerai bisa kah ?',\n",
    "       '@jamal_alfkiir bilang nya mau beli promo papi duo?',\n",
    "       'Ultimate cheese ini enak menurutku', 'Promonya sampai kapan ka?',\n",
    "       '@dominos_id oke mksh infonya ka', 'Itu 3 brp min?',\n",
    "       'Ultimate Cheese', 'Ultimate cheese pizza',\n",
    "       'Tp punya ku td malam beli ga semelimpah itu😢',\n",
    "       \"please @dominos_id improve your service at your kalibata city's outlet!!!\",\n",
    "       'Berapa itu satuan',\n",
    "       '@dominos_id wawww seru kalau makan bareng keluarga😮',\n",
    "       'Papi trio yg mana ka',\n",
    "       '@dominos_id coba yg no 2 beda sm yg ada di video',\n",
    "       'American pie biasa aja 😂', 'Apa ini ga ada volcanoooo',\n",
    "       '@dominos_id tuna delight ini beneran discontinue apa nggak ada sementara aja? 😞😞😞😞 pizza favorit padahal',\n",
    "       'Brp hrg',\n",
    "       '@dominos_id kenapa ga ada min padahal enakan tuna deligjt loh',\n",
    "       'Papi trio masih ada?',\n",
    "       'Spreme cheese pie ini pav anakku tolong @dominos_id jgn ilangin menu ini.. anakku gtm anakku dirawat di RS maunya cuma makan domino supreme cheese aja',\n",
    "       'No 1 dan no 3 setujuuu👏👏👏👏👏', '@henhenhenhens enakan mana kak?',\n",
    "       '@dominos_id bedanya ultimate cheese melt dan pie supreme cheese apa kak?',\n",
    "       '@verenaprilianii klo suka kejuuuuu banget, saran coba yg ultimate cheese',\n",
    "       'min papi viral gada lagi ya?', 'Berapa ?',\n",
    "       'Min papi duo masi berlaku?',\n",
    "       '@dominos_id ini ukurannya seberapa min??',\n",
    "       '@dominos_ ini ukuran seberapa min??',\n",
    "       'Masih ada engk promo pizza yg ini min?',\n",
    "       '@dominos_id cusss pesen kakak😍😍',\n",
    "       'Udah semua yg paling fav bgt yg nomor 2 OMG enak bgttt udah sering bgt pesan itu❤️❤️❤️',\n",
    "       'Brapa ya kak', '@dominos_id hari ini masih promo ga kak ?',\n",
    "       'Apa promo masih ada',\n",
    "       'Kalau makan di tempat gk habis bisa take way gk kk.?',\n",
    "       'Papi duo medium cuma dapet dua pizza. Ada paketnya g min yang isinya tiga varian ini ?',\n",
    "       '@dominos_id kalau ketigah tigahnya berapa kk',\n",
    "       'Kalau ketigah tigahnya berapa kk.?', 'Brp an kak', 'Enak semuaa🔥',\n",
    "       'Ga ada promo beli 2 dg potongan harga ya?',\n",
    "       '@dominos_id papi duo rasa pizza nya bisa milih atau gimana?',\n",
    "       'Spil harganya kk', '@dominos_id tnks kk',\n",
    "       '@dominos_id papi duo medium itu apa', 'So yummy',\n",
    "       'ih yang ini nahh 85 ribu sih 😆😢 @lillah1898',\n",
    "       '@hannalillah18 gasss 🤭',\n",
    "       'Enakk banget .. aku yang gak doyan pizza aja doyan banget sama pizza domino chesse nya enak gak bikin eneg .. 😍',\n",
    "       'Please yg ini berapaan?',\n",
    "       '@alineoktaviane wah terimakasih min infonya, pulang kerja cuss ke dominos',\n",
    "       'Kok pengen @trippinjann',\n",
    "       '@dikaisn iyh domino sama pizzahut juga sama',\n",
    "       'Aku psen yang ini yang large bner kaya roti biasa topingnya dikit bgt...',\n",
    "       'Beda dengan yang medium bnyak kejunya...knpa bisa sperti itu ya???? Saya mrasa rugi aja gt',\n",
    "       'Ticket number 10589380... not resolved yet',\n",
    "       'Yang 3 pizza pinggir nya ada kejunya itu apa ya nama pizza nya dl pernah pernah pesen cuma lupa namanya',\n",
    "       '@dominos_id skrng sudah tidak ada ya kok saya liat di app tidak ada',\n",
    "       '@dominos_id kon di apo gk ada ya skrng pizza itu cheese burst dulu saya beli pizza yg dapet 3 itu',\n",
    "       '@ratnarendiany gass teh ngiler 🤤', '@eka_puspita hayuuu gas ken',\n",
    "       '@ratnarendiany gaasss',\n",
    "       'Kmren beli langsung di outletnya 90 rebuan, ko di shope food cuma 50 rebu . Itu bedanya apa yak, beda harganya jauh bgt',\n",
    "       'kira kira crust yang cocok apa buat ultimate cheese melt',\n",
    "       '@dominos_id kalo ultimate meat overload sama volcano cheese crust nya ht juga min cocok nya?',\n",
    "       'Wenakkk rekkkk',\n",
    "       '@dominos_id tadi baru aja beli,enakk bangett nagih❤️❤️🔥',\n",
    "       'Chease',\n",
    "       'beda dari aslinya😂 udah beli malah kejunya dikit banget jadi nyesel beli nyaa',\n",
    "       '@dominos_id di dm ga dibalass', 'Paling suka yg ini😍',\n",
    "       '😢❤️🔥🙌👏😍😮😂😂😂😂',\n",
    "       'INI ENAK BGT SUMPAH SAMA YG VOLCANO JG ENAK BGT YA ALLAH PGN NANGISSS MAU LAGI BSK MAU BELI LAGIIIII😭😭😭😭❤️❤️❤️❤️❤️❤️',\n",
    "       'Gara² menu ini seminggu bs 3x pesen. Efeksamping BB naikk 😬',\n",
    "       '@fatricia_budiaty huuuu 😭😭😭',\n",
    "       '@rika_yanti789 iya kmren w jg beli tp kejunya ga luber kya di iklan. Yg di iklan itumah kejunya udh tambahan jdi harganya jg nambah',\n",
    "       'Sangat kecewa pizzanya tiba sudah dingin, klaimnya \"lebih dari 30menit Free\" di Apps nya tapi tidak terbukti, dan DM di IG inipun sudah 1 minggu lebih tidak ada respon',\n",
    "       'Saya order melalui app, dari azan isya sampe jam 9 blm sampai pesanan status di app delivery,',\n",
    "       'Pesanan di dominos pos pengumben',\n",
    "       '@seravinanathania aku dapetnya kering😢apa Krn udah dingin ya',\n",
    "       'Kalo di video beda sama di realita.. mozanya gak sebanyak ituu',\n",
    "       'Favorite banget',\n",
    "       '@dominos_id dimana vouchernya gada voucher sama sekali',\n",
    "       'Ini masih', 'Favorit sih ini enak bgt',\n",
    "       '@@dominos_id di online kaya grab dan gofood, ultimate cheese ada yg 83,85 dan 103rb beda nya apa?',\n",
    "       '@ajadspk_ pgnnnn iniiiiii',\n",
    "       'Saya pesan via aplikasi nya, tp gak dikasih struk nya sm bang kurirnya.. Pdhl kan mau ikutan event nya.. Parah sih',\n",
    "       'Fav aku nih', 'Enak bgt kemaren udah ke kelapa gading 😍',\n",
    "       'Ada porsi kecil ga ?pengen tapi sendirian', 'Brp harganya',\n",
    "       '𝖡𝗂𝗌𝖺 𝗅𝖾𝗐𝖺𝗍 𝗀𝗋𝖺𝖻 𝖿𝗈𝗈𝖽 𝗀𝗄 𝗄𝖺𝗄', 'Enakk parahh',\n",
    "       '@dominos_id promo papi duo pembelian dimana kak',\n",
    "       '@dominos_id nggk bisa sama skali', 'Harga brp stlh diskon min?',\n",
    "       'Jd kena brp min?', 'min cek dm dong', 'Kepengen 🤤@hafiznafilah',\n",
    "       '@dominos_id sampai kapan ?', 'Terbaeekkk nih favorit aku 🤩🤩🤩',\n",
    "       'La ini tu promosi yang gak bikin ribet 😍😍',\n",
    "       '@dominos_id dpt brp box pitza kak dan varian ap yaaa',\n",
    "       'Kalo lewat app cara mesennya gimana sih min',\n",
    "       'Pesan gimana biar sama persis gtu? Soalnya banyak yg komen isiannya gk sesuai 😂',\n",
    "       '@dominos_id kalau yg promo papi duo itu persis ky gtu gk min?',\n",
    "       'Masih discon gak?', '@dominos_id diskon smp kapan kak',\n",
    "       'Yg di video itu pizza apa aja ya min?',\n",
    "       'Masih promosi gak yaang pie supreme cheese min @dominos_id',\n",
    "       '@dominos_id untuk pembelian 1 atau 3 min', 'Berapaan min?',\n",
    "       '@dominos_id sampai kapan KK promonya?',\n",
    "       '60rb itu udh diskon 30%?',\n",
    "       'Ka bisa delivery ga ? Atau harus takeaway lewat apk',\n",
    "       '@dominos_id tapi tapi tapiii saya beli yang 100an kena ongkir min..',\n",
    "       '@dominos_id Applikasi domino nya kak min', 'pizza kecil itu',\n",
    "       'Sampai kapan min promonya', 'Mauuu paket nya apa nama nya..kk',\n",
    "       'Di jember ada gak?', 'Bln niv masih berlaku ga',\n",
    "       'Gimna cara ordernya kk', 'Sampe tanggal berapa min?',\n",
    "       '@valerian_evander mending ini nyet',\n",
    "       'saya udh download tpi gak bisa ka..', 'Sudah sama pajak kk?',\n",
    "       '@dominos_id jadi brpa kk', '@halidddd_ emg lu doyan pizza?',\n",
    "       'Di ambil sendri ke store atau delivery y min?',\n",
    "       'Makasih domino udah ciptain paket ini,berkat paket ini aku bisa teraktir teman 1 team ku karena murce tapi keliatan wah,edisi ulta akoh 😍😍',\n",
    "       'Beli ini syg @dhy.sn12', '@dominos_id oke kak, thanks yaaa☺☺',\n",
    "       'ultimate cheese ini enak menurutku'\n",
    "       '😮1kotak pizza 4potong.. maksudnya 5kotak gitu', 'Larange']\n",
    "\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "class_labels = predict_sentiment_batch(preprocessed_texts)\n",
    "\n",
    "# class_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "exclude_words = ['gua', 'kak', 'gue', 'sih', 'kasih', 'banget', 'orang', 'bu', 'sumpah', 'gitu', 'bnyak', 'banyak', 'gt', 'gitu', 'duo', 'dua', 'satu', 'min', 'pesen', 'brp', 'berapa','memang', 'mmg', 'udh', 'udah', 'uda', 'niat', 'tp', 'tapi', 'menit']\n",
    "\n",
    "pos_counter = Counter()\n",
    "neg_counter = Counter()\n",
    "\n",
    "for text, label in zip(preprocessed_texts, class_labels):\n",
    "    # Positive\n",
    "    if label == 2:  \n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                # Check if the word is a noun or in the custom noun dictionary\n",
    "                word_text = word.text\n",
    "                if word_text in exclude_words:\n",
    "                    previous_noun = None\n",
    "                    continue\n",
    "                if (word.upos == \"NOUN\" or word_text in nouns):\n",
    "                    previous_noun = word_text\n",
    "                # Check for adjective and combine with previous noun\n",
    "                elif (word.upos == \"ADJ\" or word_text in adjectives) and previous_noun:\n",
    "                    phrase = f\"{previous_noun} {word_text}\"\n",
    "                    pos_counter[phrase] += 1\n",
    "                    previous_noun = None  # Reset previous noun after pairing\n",
    "                # if word.upos == \"ADJ\" and word.head > 0:\n",
    "                #     head_word = sent.words[word.head - 1]\n",
    "                #     if head_word.upos == \"NOUN\" and head_word.text.lower() not in exclude_words:\n",
    "                #         phrase = f\"{head_word.text.lower()} {word.text.lower()}\"\n",
    "                #         pos_counter[phrase] += 1\n",
    "                # if word.upos == \"NOUN\" and word.text.lower() not in exclude_words: \n",
    "                #     pos_counter[word.text.lower()] += 1\n",
    "    # Negative\n",
    "    elif label == 0:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                word_text = word.text\n",
    "                if word_text in exclude_words:\n",
    "                    previous_noun = None\n",
    "                    continue\n",
    "                if word.upos == \"NOUN\" or word_text in nouns:\n",
    "                    previous_noun = word_text\n",
    "                # Check for adjective and combine with previous noun\n",
    "                elif (word.upos == \"ADJ\" or word_text in adjectives) and previous_noun:\n",
    "                    phrase = f\"{previous_noun} {word_text}\"\n",
    "                    neg_counter[phrase] += 1\n",
    "                    previous_noun = None  # Reset previous noun after pairing\n",
    "                # if word.upos == \"ADJ\" and word.head > 0:\n",
    "                #     head_word = sent.words[word.head - 1]\n",
    "                #     if head_word.upos == \"NOUN\" and head_word.text.lower() not in exclude_words:\n",
    "                #         phrase = f\"{head_word.text.lower()} {word.text.lower()}\"\n",
    "                #         neg_counter[phrase] += 1\n",
    "                # if word.upos == \"NOUN\" and word.text.lower() not in exclude_words:  \n",
    "                #     neg_counter[word.text.lower()] += 1\n",
    "pos_common_words = pos_counter.most_common()\n",
    "neg_common_words = neg_counter.most_common()\n",
    "print(\"Yang disukai customer:\", pos_common_words)\n",
    "print(\"Yang tidak disukai customer:\", neg_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step\n",
      "(['cheese enak', 'cheese kaya', 'video sedih'], ['ai parah', 'pelayanan buruk', 'oulet mahal'])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_sentiment(texts):\n",
    "    # Preprocess texts and predict sentiment\n",
    "    preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "    class_labels = predict_sentiment_batch(preprocessed_texts)\n",
    "    \n",
    "    # Initialize counters for positive and negative sentiment words\n",
    "    pos_counter = Counter()\n",
    "    neg_counter = Counter()\n",
    "\n",
    "    # Analyze the texts based on sentiment labels\n",
    "    for text, label in zip(preprocessed_texts, class_labels):\n",
    "        previous_noun = None\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                word_text = word.text\n",
    "                \n",
    "                # Skip words in exclude_words list\n",
    "                if word_text in exclude_words:\n",
    "                    previous_noun = None\n",
    "                    continue\n",
    "                \n",
    "                # Check for noun or custom nouns\n",
    "                if word.upos == \"NOUN\" or word_text in nouns:\n",
    "                    previous_noun = word_text\n",
    "                \n",
    "                # Check for adjective and combine with previous noun\n",
    "                elif (word.upos == \"ADJ\" or word_text in adjectives) and previous_noun:\n",
    "                    phrase = f\"{previous_noun} {word_text}\"\n",
    "                    if label == 2:  # Positive sentiment\n",
    "                        pos_counter[phrase] += 1\n",
    "                    elif label == 0:  # Negative sentiment\n",
    "                        neg_counter[phrase] += 1\n",
    "                    previous_noun = None  # Reset previous noun after pairing\n",
    "\n",
    "    # Get the most common words\n",
    "    pos_common_words = pos_counter.most_common()\n",
    "    neg_common_words = neg_counter.most_common()\n",
    "\n",
    "    # Print results\n",
    "    # print(\"Yang disukai customer:\", pos_common_words)\n",
    "    # print(\"Yang tidak disukai customer:\", neg_common_words)\n",
    "    \n",
    "    return pos_common_words, neg_common_words\n",
    "\n",
    "def get_top_3_common_words(pos_common_words, neg_common_words):\n",
    "    top_3_pos_words = [word for word, _ in pos_common_words[:3]]\n",
    "    top_3_neg_words = [word for word, _ in neg_common_words[:3]]\n",
    "    \n",
    "    return top_3_pos_words, top_3_neg_words\n",
    "\n",
    "def analyze_sentiment_top_3(texts):\n",
    "    pos_common_words, neg_common_words = analyze_sentiment(texts)\n",
    "    return get_top_3_common_words(pos_common_words, neg_common_words)\n",
    "\n",
    "print(analyze_sentiment_top_3(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_noun_tags = {\"cheese\"}  # Add words you know are nouns but may not be detected.\n",
    "\n",
    "def extract_adj_noun_phrases(texts):\n",
    "    phrases_counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            previous_noun = None\n",
    "            for word in sent.words:\n",
    "                word_text = word.text.lower()\n",
    "                # Check if the word is a noun or in the custom noun dictionary\n",
    "                if word.upos == \"NOUN\" or word_text in nouns:\n",
    "                    previous_noun = word_text\n",
    "                # Check for adjective and combine with previous noun\n",
    "                elif (word.upos == \"ADJ\" or word_text in adjectives) and previous_noun:\n",
    "                    phrase = f\"{previous_noun} {word_text}\"\n",
    "                    phrases_counter[phrase] += 1\n",
    "                    previous_noun = None  # Reset previous noun after pairing\n",
    "\n",
    "    return phrases_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common adjective-noun phrases: []\n"
     ]
    }
   ],
   "source": [
    "texts = ['ultimate cheese ini nggak enak menurutku']\n",
    "texts = [preprocess_text(text) for text in texts]\n",
    "# texts\n",
    "adj_noun_phrases = extract_adj_noun_phrases(texts)\n",
    "print(\"Most common adjective-noun phrases:\", adj_noun_phrases.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common adjective-noun phrases: [('makanan enak', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Example texts\n",
    "texts = [\n",
    "    'ultimate cheese enak menurutku',\n",
    "    'makanan enak banget tai'\n",
    "]\n",
    "\n",
    "# Define custom words to exclude\n",
    "exclude_words = {\"tapi\", \"dan\", \"sangat\"}\n",
    "\n",
    "# Function to extract adjective-noun pairs\n",
    "def extract_adj_noun_phrases(texts):\n",
    "    phrases_counter = Counter()\n",
    "    \n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sentences:\n",
    "            for word in sent.words:\n",
    "                # Find adjective (ADJ) modifying a noun (NOUN)\n",
    "                if word.upos == \"ADJ\" and word.head > 0:\n",
    "                    head_word = sent.words[word.head - 1]\n",
    "                    if head_word.upos == \"NOUN\" and head_word.text.lower() not in exclude_words:\n",
    "                        phrase = f\"{head_word.text.lower()} {word.text.lower()}\"\n",
    "                        phrases_counter[phrase] += 1\n",
    "\n",
    "    return phrases_counter\n",
    "\n",
    "# Get adjective-noun phrases\n",
    "adj_noun_phrases = extract_adj_noun_phrases(texts)\n",
    "\n",
    "# Print most common phrases\n",
    "most_common_phrases = adj_noun_phrases.most_common()\n",
    "print(\"Most common adjective-noun phrases:\", most_common_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
